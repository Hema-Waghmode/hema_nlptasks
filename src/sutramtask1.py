# -*- coding: utf-8 -*-
"""SutramTask1.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1HZg9EeKbd_GYbSbduBdqfTqhSK5PsDx2
"""

pip install tensorflow nltk

import numpy as np
import tensorflow as tf
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Embedding, SimpleRNN, Dense
from tensorflow.keras.preprocessing.text import Tokenizer
from tensorflow.keras.preprocessing.sequence import pad_sequences
import random

# Sample educational data
classification_texts = [
    ("The area of circle is area = py*r*r", "Math"),
    ("A region around a charged object, where another object experiences a force is called an electric field.", "Science"),
    ("The Mughals briefly occupied Pune, renaming it Muhiyabad", "History"),
    ("English is an Indo-European language, primarily spoken in the United Kingdom, the United States, and other Commonwealth countries", "English"),

]

generation_sentences = [
    "A circle is also defined by two of its properties, such as area and perimeter",
    "The electric field is a vector quantity that has both magnitude and direction. The electric fields are visually represented by the electric field lines",
    "Pune was under British control after 1817, becoming the monsoon capital of the Bombay Presidency",
    "English emerged from Anglo-Saxon dialects in what is now England",

]

# Map labels to integers
label_map = {"Math": 0, "Science": 1, "History": 2, "English": 3}
classification_texts_x = [x[0] for x in classification_texts]
classification_texts_x
classification_texts_y = [label_map[x[1]] for x in classification_texts]

classification_texts_x

classification_texts_y

# Combine all for tokenization
all_text = classification_texts_x + generation_sentences

all_text

tokenizer = Tokenizer()
tokenizer.fit_on_texts(all_text)
vocab_size = len(tokenizer.word_index) + 1

# Encode classification input
X_class = tokenizer.texts_to_sequences(classification_texts_x)
X_class = pad_sequences(X_class, padding='post', maxlen=20)
y_class = tf.keras.utils.to_categorical(classification_texts_y, num_classes=4)

# Prepare sequences for word generation
input_sequences = []
for line in generation_sentences:
    tokens = tokenizer.texts_to_sequences([line])[0]
    for i in range(10, len(tokens)):
        n_gram_seq = tokens[:i]
        input_sequences.append(n_gram_seq)

# Pad for equal input length
max_seq_len = max([len(x) for x in input_sequences])
input_sequences = pad_sequences(input_sequences, maxlen=max_seq_len, padding='pre')

# Split into input and label
X_gen = input_sequences[:, :-1]
y_gen = tf.keras.utils.to_categorical(input_sequences[:, -1], num_classes=vocab_size)

#sared model architechture
def build_rnn_model(output_units, output_activation):
    model = Sequential([
        Embedding(input_dim=vocab_size, output_dim=50, input_length=max_seq_len-1),
        SimpleRNN(64),
        Dense(output_units, activation=output_activation)
    ])
    return model

#train for classification
model_class = build_rnn_model(output_units=4, output_activation='softmax')
model_class.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])
model_class.fit(X_class, y_class, epochs=10)

#train for next word
model_gen = build_rnn_model(output_units=vocab_size, output_activation='softmax')
model_gen.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])
model_gen.fit(X_gen, y_gen, epochs=20)

#word prediction fun
def generate_text(seed_text, next_words=20):
    for _ in range(next_words):
        token_list = tokenizer.texts_to_sequences([seed_text])[0]
        token_list = pad_sequences([token_list], maxlen=max_seq_len-1, padding='pre')
        predicted = np.argmax(model_gen.predict(token_list, verbose=0), axis=-1)
        output_word = ''
        for word, index in tokenizer.word_index.items():
            if index == predicted:
                output_word = word
                break
        seed_text += ' ' + output_word
    return seed_text

#Example
# Classification
test_text = "As of 2020, there were over 1.27 billion English speakers worldwide"
test_seq = tokenizer.texts_to_sequences([test_text])
test_seq = pad_sequences(test_seq, maxlen=20, padding='post')
predicted_label = np.argmax(model_class.predict(test_seq), axis=-1)
print("Predicted class:", list(label_map.keys())[list(label_map.values()).index(predicted_label[0])])

# Next Word Generation
print(generate_text("Generated Text:"))